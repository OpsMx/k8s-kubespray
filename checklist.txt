Steps to configure K8s-Cluster using Ansible Scripts and Kubespray

Step#1: creating passwordless set up
login to a secific user and generate ssh public keys
	$ ssh-keygen -t rsa
	$ cat /home/<user>/.ssh/id_rsa.pub
login to a 'root' user and generate ssh public keys
	$ ssh-keygen -t rsa
	$ cat /root/.ssh/id_rsa.pub	
List out the above generated keys and note down to a file.
Copy the public-key(generated above) into all the machines 
	$ vi .ssh/authorized_keys
	$ vi /root/.ssh/authorized_keys
Test the connectvity among the machines.
	$ ssh -A IPs ( all private IPs, or with hostnames)
In all machines of /etc/hosts update all master and cluster nodes hostnames with IPs
In all machines update /etc/hostname file with its Fully Qualified Domain Name ( like kubemaster1.opsmx.com)
In all machines update /etc/resolve.conf with its DNS server address. ( nameserver < DNS server IP >, search < your domain name > )
	
Ansible Installation:
	In current setup we are using HAProxy system as our launch machine

Step #2: Create Virtual environment and Install Ansible in launch machine
Run the below commands for virtual setup and run the below commands for settingup virtual environment:
	$ sudo apt-get update
	$ sudo apt-get install -y python-pip
	$ sudo pip install  virtualenv
	
	$ virtualenv ~/ansible
	$ . ~/ansible/bin/activate
	>>>>(ansible) opsmxgcetest@cisco-kubemhanfs:
		
Install Ansible in the launch machine:
	$ sh ansible-setup.sh 
	or run the following commands in sequence
	$ sudo apt-get update
	$ sudo apt-get install -y software-properties-common
	$ sudo apt-add-repository ppa:ansible/ansible
	$ sudo apt-get update
	$ sudo apt-get install -y ansible
	$ sudo apt-get install -y python
	$ sudo apt-get install -y python-netaddr
	$ sudo apt-get install -y jinja2
	or 
	$ sudo apt-get update
	$ sudo apt-get install -y software-properties-common
	$ sudo apt-add-repository ppa:ansible/ansible
	$ sudo apt-get update
	$ Sudo apt-get install -y python-pip
	$ sudo pip install -y ansible
	$ sudo pip install -y python-netaddr
	$ sudo pip install -y jinja2
	
Run the below command in all the machines (Master & Non-master):
	$ sudo apt-get update && sudo apt-get install -y python	//This is required for ansible connectvity in all machines
	$ sudo apt-get update && sudo apt-get install -y python-netaddr	//This is required for ansible-playbook in ansible launch machine
	
Update /etc/ansible/hosts file 	ref : etc-ansible-hosts.txt
	Check all the machines connectivity using ansible ping
	$ ansible -m ping all -- Or can perform it with individual nodes
	

git clone in launch machine:
	$ git clone https://github.com/OpsMx/k8s-kubespray

Perform ssh testing using ansible-playbook
	copy 'id_rsa' to the location of ssh.yml for checking ssh.
	$ ansible-playbook k8s-kubespray/kubespray-install/ssh.yml

Perform haproxy testing	using ansible-playbook
	$ ansible-playbook k8s-kubespray/kubespray-install/haproxy/haproxy.yml


Edit the below file: ( Should look like the below )
	k8s-kubespray/kubespray-install/default/group_vars/all/all.yml
	### Update Master & Load Balance IPs with the below keys array:
	### supplementary_addresses_in_ssl_keys		//Public and Private IPs
	ansible_ssh_extra_args: '-o StrictHostKeyChecking=no'
	kubeconfig_localhost: true
	supplementary_addresses_in_ssl_keys: [35.234.132.158,10.154.0.16,10.154.0.17]
	
	cloud_provider: gce
	etcd_data_dir: /var/lib/etcd
	bin_dir: /usr/local/bin
	
	loadbalancer_apiserver_localhost: true
	nginx_kube_apiserver_port: 6443
	

	loadbalancer_apiserver_localhost: true
	upstream_dns_servers:
	 - 8.8.8.8
	cert_management: scrip
	kube_read_only_port: 10255

Configure 'Enable Ingress Controller Deployment'
Verify the below things in k8s-kubespray/kubespray-install/default/group_vars/k8s-cluster/addons.yml
	# Nginx ingress controller deployment
	ingress_nginx_enabled: true
	ingress_nginx_host_network: false
	ingress_nginx_nodeselector:
	  node-role.kubernetes.io/node: ""
	
For addition, enabling 'helm'.
	helm_enabled: true
	helm_version: v2.13.1
	
edit 'k8s-kubespray/kubespray/roles/kubernetes-apps/ingress-controller/ingress-nginx/templates/svc-default-backend.yml.j2' and ensure the below:
	labels:
		app.kubernetes.io/name: ingress-nginx
		app.kubernetes.io/part-of: ingress-nginx
	spec:
	  type: NodePort
	  ports:
		- port: 80
		  targetPort: 80
	  selector:
		app.kubernetes.io/name: ingress-nginx
		app.kubernetes.io/part-of: ingress-nginx
		
Note: Kubectl binaries and helm need to be downloaded and set in binary path
	 for that we following below steps
## Kubect binary installation
curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl

## Helm installation
curl -sLO https://storage.googleapis.com/kubernetes-helm/helm-v2.13.1-linux-amd64.tar.gz
tar xfz helm-v2.13.1-linux-amd64.tar.gz
sudo mv linux-amd64/helm /usr/local/bin/helm

Ensure the below lines in 'k8s-kubespray/kubespray-install/default/group_vars/k8s-cluster/k8s-cluster.yml'
	kube_oidc_auth: true
	kube_oidc_url: https://dex.k8s.example.com:32000
	kube_oidc_client_id: loginapp
	kube_oidc_ca_file: "{{ kube_cert_dir }}/ca.pem"
	kube_oidc_username_claim: name
	kube_oidc_groups_claim: groups
	manual_dns_server: 1.1.1.1
	### Provide API server IPs with Pubic-IPs
	kube_apiserver_ip: 35.246.52.137 35.234.132.158 35.246.120.230 35.234.158.132


Now we can run ansible play book
	$ansible-playbook -b k8s-kubespray/kubespray/cluster.yml   -- It will take five minutes or more to complete
	
After cluster completion to test perform the steps below
	copy content of /etc/kunernetes/admin.conf from master node1 to kubemaster.conf in launch machine
	Then run the following commands to veryfy the pods and nodes
	
	kubectl get node --kubeconfig=kubemaster.conf
		NAME        STATUS   ROLES         AGE    VERSION
		kubeubum1   Ready    master        3d7h   v1.13.5
		kubeubum2   Ready    master,node   3d7h   v1.13.5
		kubeubum3   Ready    master,node   3d7h   v1.13.5
	kubectl get namespaces --kubeconfig=kubemaster.conf
	kubectl get pods --all-namespaces --kubeconfig=kubemaster.conf





	

	



	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
